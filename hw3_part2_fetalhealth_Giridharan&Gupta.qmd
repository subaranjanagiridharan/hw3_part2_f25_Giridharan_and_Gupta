---
title: "HW3-Part 2-Classification models"
author: "Subaranjana Giridharan & Archit Gupta"
date: "November 08, 2025"
format: 
  html:
    embed-resources: true
---

## Step 1 - Familiarize yourself with the data and the assignment

Save this file as a new Quarto Markdown document and name it something that
includes your last name in the filename. Save it into the
same folder as this file. Create a new R Studio Project based on this folder.

This assignment will focus on building simple classification models for
predicting fetal health based on a number of clinical measurements known
as *cardiotocographic data*. 

The following introductory information was taken from the main
[Kaggle Dataset page](https://www.kaggle.com/andrewmvd/fetal-health-classification) for this data:


>    **Context**

> Reduction of child mortality is reflected in several of the United Nations' Sustainable Development Goals and is a key indicator of human progress. The UN expects that by 2030, countries end preventable deaths of newborns and children under 5 years of age, with all countries aiming to reduce under‑5 mortality to at least as low as 25 per 1,000 live births.
> 
Parallel to notion of child mortality is of course maternal mortality, which accounts for 295 000 deaths during and following pregnancy and childbirth (as of 2017). The vast majority of these deaths (94%) occurred in low-resource settings, and most could have been prevented.
>
> In light of what was mentioned above, Cardiotocograms (CTGs) are a simple and cost accessible option to assess fetal health, allowing healthcare professionals to take action in order to prevent child and maternal mortality. The equipment itself works by sending ultrasound pulses and reading its response, thus shedding light on fetal heart rate (FHR), fetal movements, uterine contractions and more.
>
> **Data**
>
> This dataset contains 2126 records of features extracted from Cardiotocogram exams, which were then classified by three expert obstetritians into 3 classes:

> * Normal
> * Suspect
> * Pathological

The definitions of the columns are:

    * baseline value - FHR baseline (beats per minute)
    * accelerations - Number of accelerations per second
    * fetal_movement - Number of fetal movements per second
    * uterine_contractions - Number of uterine contractions per second
    * light_decelerations - Number of light decelerations per second
    * severe_decelerations - Number of severe decelerations per second
    * prolongued_decelerations - Number of prolonged decelerations per second
    * abnormal_short_term_variability - Percentage of time with abnormal short term variability
    * mean_value_of_short_term_variability - Mean value of short term variability
    * percentage_of_time_with_abnormal_long_term_variability - Percentage of time with abnormal long term variability
    * mean_value_of_long_term_variability - Mean value of long term variability
    * histogram_width - Width of FHR histogram
    * histogram_min - Minimum (low frequency) of FHR histogram
    * histogram_max - Maximum (high frequency) of FHR histogram
    * histogram_number_of_peaks - Number of histogram peaks
    * histogram_number_of_zeroes - Number of histogram zeros
    * histogram_mode - Histogram mode
    * histogram_mean - Histogram mean
    * histogram_median - Histogram median
    * histogram_variance - Histogram variance
    * histogram_tendency - Histogram tendency

You can learn much more about the study behind this dataset from the following
published paper:

* https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6822315/

While in the original study and dataset, there were three classes for the
target variable (Normal, Suspect, Pathological), we will convert this to
a binary classification problem where 1 will be Suspect or Pathological and
0 will be Normal. Multi-class problems are a bit beyond the scope of this
introduction to classification problems.

```{r libraries}
library(dplyr)   # Group by analysis and other SQLish things.
library(ggplot2) # Plotting, of course
library(tidyr)   # Data reshaping
library(tidymodels)   # Many aspects of predictive modeling
library(corrplot)  # Correlation plots
library(skimr)       # An automated EDA tool (you saw this in a previous assignment)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(vip)
```

## Step 2 - Load data

You'll notice that there is a subfolder named **data**.
Inside of it you'll find the data file for this assignment:

- **fetal_health.csv**


### Load the data

```{r load_data}
fetal_health <- read.csv("./data/fetal_health.csv")
```

Our target variable will be based on the `fetal_health` variable. Let's check the current values.

```{r}
table(fetal_health$fetal_health)
```

Now we will recode the 1's as a 0 and the 2's and 3's as a 1. I'm going to do this in a new column
so that we can check and make sure we've got it right.

```{r}
fetal_health$b_fetal_health <- fetal_health$fetal_health
fetal_health$b_fetal_health[fetal_health$b_fetal_health == 1] <- 0
fetal_health$b_fetal_health[fetal_health$b_fetal_health >= 2] <- 1

table(fetal_health$b_fetal_health, fetal_health$fetal_health)
```

Looks good, let's drop the original `fetal_health` column and convert the b_fetal_health` column
to a factor. I'm explicitly setting the order of the factor levels so that "0" is the first level just
so there's no confusion.

```{r}
fetal_health <- fetal_health  |>  
  select (!fetal_health) 

fetal_health$b_fetal_health <- factor(fetal_health$b_fetal_health, levels=c("0", "1"))
```

Find the number of patients and the percentage of patients for the two fetal health levels - 1 and 2. You'll
see that there are about 78% of the patients with a normal fetal health assessment (i.e. `b_fetal_health` = 0)

```{r target_prop_check}
count_fhl <- table(fetal_health$b_fetal_health)
count_fhl
prop_fhl <- prop.table(count_fhl) * 100
round(prop_fhl, 2)
```

Use `str`, `summary`, and `skim` to get a sense of the data. Our response variable, the thing we will be trying to predict is `b_fetal_health`. 

```{r firstlook}
str(fetal_health)
```



```{r}
summary(fetal_health)
```

```{r}
skim(fetal_health)
```

## Step 3 - Data partitioning

Ok, it's time to do an initial split of our data into training and test datasets.

For data partitioning, we will use the [rsample](https://rsample.tidymodels.org/) package. Let's allocate 80% of the records to our training set and 20% to the test set. Make sure you specify the `strata = ` argument. After partitioning, confirm that the `b_fetal_health` proportions of 1's and 0's are as expected. Discuss.

```{r partitioning_soln}
set.seed(983)
pct_train <- 0.80
 
# Create a "split object" based on an 80/20, stratified split. 
fetal_health_split <- rsample::initial_split(fetal_health, 
                                         prop = pct_train,
                                         strata = b_fetal_health)
 
# Create train and test dataframes based on the split
fetal_health_train <- training(fetal_health_split)
fetal_health_test <- testing(fetal_health_split)
```


## Step 4 - EDA

Do some EDA to try to uncover some relationships that may end up being
useful in building a predictive model for `b_fetal_health`. You can only use the training data for this. You learned
things in HW2 which should be useful here. In particular, you should at least do:

- a correlation plot
- faceted box or violin plots for all of the numeric variables with the target variable as the grouping variable,

I'll give you a little help for each of these.

### Correlation plot

> We'll start with a correlation plot. The `tl.cex` argument helps us display the 
plot by controlling the size of the text label. It's behavior is a little unpredictable but you can play around with various values. 

```{r corrplot}
numeric_train <- fetal_health_train |> select(where(is.numeric))

cormat <- cor(numeric_train, use = "pairwise.complete.obs")

corrplot::corrplot (cormat, type   = "lower", order  = "hclust",
  tl.cex = 0.5, cl.cex = 0.5, tl.col = "black")
```

> Positive coorelation is in histogram_value, histogram_mode, histogram_mean, histogram_median, histogram_width and histogram_number_of_peaks. Negative coorelation is cluster is in histogram_mean, mean_value_of_short_term_variability, light_decelerations, histogram_variance, histogram_width and histogram_number_of_peaks.

Now make a bunch of boxplots (or violin) that are grouped by the target variable. There are a few ways we can do this.
 
Instead of creating separate plots for each variable, we could create
a faceted plot, where we facet by the variable. To do this, we need to reshape
our data from wide to long. We can do this using `tidyr::pivot_longer()`.

```{r reshape_longer}
fetal_health_long <- fetal_health_train |>
pivot_longer(    cols = -b_fetal_health, names_to  = "variable", values_to = "value")
```

Now we can create the faceted plot using the long data.

```{r faceted_var_plot, fig.width=12, fig.height=16}
fetal_health_long %>% 
ggplot(ggplot2::aes(x = b_fetal_health, y = value, fill = b_fetal_health)) +
geom_boxplot(outlier.alpha = 0.25) +
facet_wrap(~ variable, scales = "free_y", ncol = 4) +
labs(title = "Box plots by predictor (training set)",
x = "Fetal health (0 = normal, 1 = suspect/pathological)",
y = NULL) + guides(fill = "none") + theme_minimal(base_size = 11) +
theme(strip.text = ggplot2::element_text(size = 9)) +
ggtitle("Box plots by Predictor") +
xlab("Fetal health (0=normal)")
```

> The following variables show the clearest separation between normal and abnormal fetal health categories.

 * abnormal_short_term_variability - Higher in abnormal group
 * fetal_movement, prolonged_decelerations and severe_decelerations - Lower in abnormal group with many zeros
 * mean_value_of_short_term_variability and mean_value_of_long_term_variability - Moderate decrease in abnormal group

## Step 5 - Building and evaluation of predictive classification models

Now that you know a little more about the data, it's time to start building a
few classification models for `b_fetal_health`. We will use:

- logistic regression
- a simple decision tree
- a random forest


We will start out using overall prediction accuracy
as our metric but we might want to consider other metrics.

**QUESTION** Why might overall prediction accuracy not be the most appropriate metric to consider? What other
metrics might be important and why?

> Accuracy can be misleading when the classes are imbalanced because it does not show how well we are identifying the fetuses that may actually be in danger. Sensitivity, specificity and metrics like precision, F1 score, and AUC give us a more honest view of the model’s performance.

### The null model

A very simple model would be to simply predict that `b_fetal_health` is equal to 0. On
the training data we saw that we'd be ~78% accurate.

**QUESTION** What is the sensitivity and specificity of the null model?

> Sensitivity is 0 and specificity is 1 (100%).

So, as we begin fitting more complicated models, remember that we need to
outperform the null model to make it worth it to use more complicated models.


### Model 1 -  logistic regression model

For logistic regression, you must use tidymodels and you should see our class notes on logistic regression, in particular, the section titled "Logistic regression with tidymodels". For this modeling technique, you must:

* fit at least two different logistic regression models, each of which should have at least five variables. For your first model, use variables that you think might have predictive value based on the EDA you did. For the second model, use all of the variables.
* use 5-fold cross-validation as our resampling scheme for model fitting,
* just take the default threshold value of 0.5,
* assess the model's performance on the training data (accuracy, sensitivity, specificity, confusion matrix),
* use the model to make predictions on the test data and assess the model's performance on the test data,
* discuss the results.

**HACKER EXTRA CREDIT** Create ROC curves and compute AUC for your logistic regression models. Discuss your interpretation of these plots.

Start by creating a classification model object using the appropriate `engine = ` argument for logistic regression.

```{r glm_spec}
glm_spec <- logistic_reg(mode = "classification", engine = "glm")
```


Create a recipe for your first model which specifies the formula for your logistic regression model.

```{r recipe_1}
recipe_1 <- recipe(b_fetal_health ~ 
                     abnormal_short_term_variability +
                     percentage_of_time_with_abnormal_long_term_variability +
                     mean_value_of_short_term_variability +
                     accelerations +
                     uterine_contractions,
                   data = fetal_health_train) %>%
  step_normalize(all_predictors())
```

Now create a workflow object and the recipe and model to it.

```{r}
wf_1 <- workflows::workflow() %>%
  workflows::add_model(glm_spec) %>%
  workflows::add_recipe(recipe_1)
```

Now we'll create a folds object to use for k-crossfold validation.

```{r}
set.seed(259)
fetal_health_folds <- vfold_cv(fetal_health_train, v = 5)
ctrl_preds <- control_resamples(save_pred = TRUE, save_workflow = TRUE)
```

Now you should fit your logistic regression model using folds object we just created.

```{r}
fit_results_logreg1 <- 
  wf_1 %>% 
  fit_resamples(
    resamples = fetal_health_folds,
    control = ctrl_preds)
```

Now we can collect the metrics.

```{r}
collect_metrics(fit_results_logreg1)
```

Let's refit on entire training data set and make predictions on the test data. Recall from the notes that we use the `last_fit` function to do this. Then we can use the `yardstick` package to compute accuracy, sensitivity and specificity. 

```{r}
last_fit_logreg1 <- wf_1 %>% last_fit(fetal_health_split)

# These are prediction on the test data

final_predicted_classes_1 <- collect_predictions(last_fit_logreg1)

# Accuracy
acc_test_logreg1  <- yardstick::accuracy(
  final_predicted_classes_1,
  estimate = .pred_class,
  truth    = b_fetal_health,
  event_level = "second")
# Do similar things for sensitivity and specificity.
sens_test_logreg1 <- yardstick::sens(
  final_predicted_classes_1,
  estimate = .pred_class,
  truth    = b_fetal_health,
  event_level = "second")
spec_test_logreg1 <- yardstick::spec(
  final_predicted_classes_1,
  estimate = .pred_class,
  truth    = b_fetal_health,
  event_level = "second")

# Combine the results into a single dataframe
  
stats_test_logreg1 <- dplyr::bind_rows(
  acc_test_logreg1, sens_test_logreg1, spec_test_logreg1) %>% 
  dplyr::mutate(data = "test", model = "logreg1")

# Confusion matrix
conf_mat_test_logreg1 <- yardstick::conf_mat(
  final_predicted_classes_1,
  estimate = .pred_class,
  truth    = b_fetal_health)

# stats_test_logreg1
stats_test_logreg1
conf_mat_test_logreg1
```

> Logistic Regression Model Performance - These results indicate that the model performs well overall, with strong ability to identify normal pregnancies, but a more moderate ability to detect abnormal fetal conditions.

> Confusion Matrix - The model rarely misclassifies normal cases with only 18 false positives. However, it misses some abnormal cases with 32 false negatives.

Let's fit a second model using all of the variables.

```{r recipe_2}
recipe_2 <-
  recipe(b_fetal_health ~ ., 
           data = fetal_health_train)

# Create your workflow object and add the recipe and model
wf_2 <- workflow() %>% 
  add_model(glm_spec) %>% 
  add_recipe(recipe_2)
```

Just like you did above, fit the model and assess its performance. Compare the results to the first logistic regression model. Use as many code chunks as needed.

```{r logreg2_fit_results}
fit_results_logreg2 <- 
  wf_2 %>%
  fit_resamples(
    resamples = fetal_health_folds,
    control   = ctrl_preds,
    metrics   = metric_set(accuracy, sens, spec)
  )

# Collect training (CV) metrics
collect_metrics(fit_results_logreg2)


last_fit_logreg2 <- wf_2 %>%
  last_fit(fetal_health_split)

# Predictions on test data
final_predicted_classes_2 <- collect_predictions(last_fit_logreg2)

# Compute accuracy, sensitivity, specificity (class "1" as event)
acc_test_logreg2 <- yardstick::accuracy(final_predicted_classes_2,
  estimate = .pred_class,
  truth    = b_fetal_health,
  event_level = "second")

sens_test_logreg2 <- yardstick::sens(final_predicted_classes_2,
  estimate = .pred_class,
  truth    = b_fetal_health,
  event_level = "second")

spec_test_logreg2 <- yardstick::spec(final_predicted_classes_2,
  estimate = .pred_class,
  truth    = b_fetal_health,
  event_level = "second")

stats_test_logreg2 <- bind_rows(
  acc_test_logreg2, sens_test_logreg2, spec_test_logreg2) %>%
  mutate(data = "test", model = "logreg2")

# Confusion Matrix for Model 2
conf_mat_test_logreg2 <- yardstick::conf_mat(final_predicted_classes_2,
  estimate = .pred_class,
  truth    = b_fetal_health)

stats_test_logreg2
conf_mat_test_logreg2
```

> Logistic Regression Model Performance - This indicates that Model 2 emphasizes detecting abnormal cases.

> Confusion Matrix - The model misses some abnormal cases with 14 false negatives reduced from 32. The false positives are still low at 19.


### Model 2 - a simple decision tree

For this model, we will **NOT** use a resampling scheme (i.e. no k-fold cross-validation). We will just use our one simple train-test split. We'll use `rpart` for the engine, just as we did in the notes.

Create your model specification.

```{r tree_spec}
tree_spec <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")
```

Let's fit a tree to the training data using just a few variables so that we
can easily see what's going on.

```{r tree1_fit}
tree1_fit <- fit(tree_spec, b_fetal_health ~  
           accelerations + 
           baseline.value +
           histogram_median + +
           uterine_contractions, data = fetal_health_train)
```

Let's see what the fitted object looks like.

```{r}
tree1_fit
```

The `rpart.plot()` function lets us see the actual (upside down) tree. Since
this function only works with `rpart` objects, we need to extract it from our
`fit` object. 


```{r tree1_plot}
tree1_fit %>%
  extract_fit_engine() %>%
  rpart.plot(tweak = 1.2)
```

**QUESTION** Explain each of the values in the node in the third row from the top in the middle (light blue with values 0, 0.31 and 26%) What conditions need to be true for a case to end up in that node?

> Explanation of the Node Values

 * 0 = The predicted class for observations in this node is 0 (Normal fetal health). This means that within this node, the majority of cases are classified as normal.

 * 0.31 = The proportion of class 1 (abnormal) in this node. 31% of the cases in this node are abnormal, and 69% are normal.

 * 26% = This node contains 26% of all observations in the training set. So, just over one quarter of the data falls into this group.

> Conditions Required to Reach This Node

  * Acceleration >= 500e-6
  * uterine_contractions >= 0.0025
  * histogram_median >= 112

Now use `augment()` function along with `yardstick::sens()`, `yardstick::spec()`, and `yardstick::conf_mat()` to compute sensitivity, specificity, and the confusion matrix on the **training data**. In other words, we are just assessing how well the tree fits the data. 

Notice in the code skeleton that I'm explicitly specifying the **yardstick** package and a function from it. I'm doing that because the `spec` function name clashes with another library we have loaded - you can see this by doing a `help(spec)`. Also notice that we need `event_level = "second"` since "0" comes before "1" in the factor levels for `b_fetal_health` and it's the "1"'s that are the thing we are trying to detect (abnormal results). We need `event_level = "second" for accuracy, sensitivity and specificity calculations (we don't need it for the confusion matrix).

```{r tree1_metrics_train}
# Accuracy
acc_train_tree1 <- augment(tree1_fit, new_data = fetal_health_train) %>%
  yardstick::accuracy(
    truth = b_fetal_health, 
    estimate = .pred_class,
    event_level = "second")

# Sensitivity
sens_train_tree1 <- augment(tree1_fit, new_data = fetal_health_train) %>%
  yardstick::sens(
    truth = b_fetal_health, 
    estimate = .pred_class,
    event_level = "second")

# Specificity
spec_train_tree1 <- augment(tree1_fit, new_data = fetal_health_train) %>%
  yardstick::spec(
    truth = b_fetal_health, 
    estimate = .pred_class,
    event_level = "second")

# Combine into stats_train_tree1
stats_train_tree1 <- bind_rows(acc_train_tree1, sens_train_tree1, spec_train_tree1) %>%  mutate(data = "train", model = "tree1")

# Confusion matrix
conf_mat_train_tree1 <- augment(tree1_fit, new_data = fetal_health_train) %>%
  yardstick::conf_mat(
    truth = b_fetal_health, 
    estimate = .pred_class)

stats_train_tree1
conf_mat_train_tree1
```


**QUESTION** What is the predicted class and the probabilities of 0 and 1 for the very first row in `fetal_health_train`? **HINT** There is nothing to compute - you just need to look at the appropriate tibble and you've already used this tibble above.

```{r explore_first_row}
augment(tree1_fit, new_data = fetal_health_train) %>%
  slice(1)
```

Now, make predictions for the test data for this first tree and compute sensitivity, specificity and the confusion matrix. Remember (see notes) that you can use the `augment()` function to compute predicted values for the test data.


```{r tree1_metrics_test}
# Accuracy
acc_test_tree <- augment(tree1_fit, new_data = fetal_health_test) %>%
  yardstick::accuracy(truth = b_fetal_health, 
                      estimate = .pred_class,
                      event_level = "second")

# Sensitivity
sens_test_tree <- augment(tree1_fit, new_data = fetal_health_test) %>%
  yardstick::sens(truth = b_fetal_health, 
                  estimate = .pred_class,
                  event_level = "second")

# Specificity
spec_test_tree <- augment(tree1_fit, new_data = fetal_health_test) %>%
  yardstick::spec(truth = b_fetal_health, 
                  estimate = .pred_class,
                  event_level = "second")

# Combine the results
stats_test_tree1 <- bind_rows(acc_test_tree, sens_test_tree, spec_test_tree) |> 
  mutate(data = 'test',
         model = 'tree1')

# Confusion matrix
augment(tree1_fit, new_data = fetal_health_test) %>%
  yardstick::conf_mat(truth = b_fetal_health, estimate = .pred_class)

stats_test_tree1
```

**QUESTION** Compare the results to what you got on the training data. Did the metrics get better or worse? Is this expected? Why? Is there evidence of overfitting? 

Here's some code that could be useful in organizing and comparing the results.

```{r tree_compare_soln}
stats_tree1 <- bind_rows(stats_train_tree1, stats_test_tree1)
stats_tree1 |> 
  select(.metric, .estimate, data) |> 
  tidyr::pivot_wider(names_from = data, values_from = .estimate)
```


> The decision tree performs slightly better on the training data compared to the test data. Training accuracy is 0.892, while test accuracy is 0.880, sensitivity drops from 0.737 to 0.706, and specificity decreases only slightly from 0.936 to 0.931. Everything is modestly higher on the training set. The model generalizes fairly well, though there is still some room for improvement, particularly in identifying abnormal fetal cases.

Now, we'll clean up the workspace a bit before trying the random forest.

```{r}
rm(tree1_fit)
```

### Model 3 - Random Forest

Instead of using a simple decision tree, use a random forest. Obviously, you should now use all the variables that you think might be useful for predicting `b_fetal_health`.

* fit the model on the training data,
* assess the model's performance on the training data using the `augment()` function like we did for the decision tree,
* use the model to make predictions on the test data and assess the model's performance on the test data using the `augment()` function,
* create an importance plot to get a sense of the relative importance of the different variables,
* discuss the results

In your discussion of the results you should talk about things like:

* how accurate is the model in predicting on the test data?
* is their evidence of overfitting?
* how does the model do in terms of other metrics like sensitivity and specificity?
* how does the performance of the random forest compare to the simple decision tree?

```{r rf_spec}
rf_spec <- rand_forest(trees = 1000) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")
```

Fit the model
```{r rf_fit}
rf_fit <- fit(rf_spec, b_fetal_health ~ ., data = fetal_health_train)
```

Compute accuracy, sensitivity and specificity for the fit.

```{r rf_fit_metrics}
rf_train_preds <- augment(rf_fit, new_data = fetal_health_train)

# Accuracy
acc_train_rf <- yardstick::accuracy(rf_train_preds,
  truth = b_fetal_health,
  estimate = .pred_class,
  event_level = "second")

# Sensitivity
sens_train_rf <- yardstick::sens(rf_train_preds,
  truth = b_fetal_health,
  estimate = .pred_class,
  event_level = "second")

# Specificity
spec_train_rf <- yardstick::spec(rf_train_preds,
  truth = b_fetal_health,
  estimate = .pred_class,
  event_level = "second")

# Combine into one data frame
stats_train_rf <- bind_rows(acc_train_rf, sens_train_rf, spec_train_rf) %>%
  mutate(data = "train", model = "rf")

# Confusion matrix
cm_train_rf <- yardstick::conf_mat(rf_train_preds,
  truth = b_fetal_health,
  estimate = .pred_class)

stats_train_rf
cm_train_rf
```

Let's look at an importance plot to get a sense of which variables seem to be influential.


```{r}
vip::vip(extract_fit_engine(rf_fit))
```
Now let's compute accuracy, sensitivity and specificity on the test data.

```{r rf_test_metrics}
rf_test_preds <- augment(rf_fit, new_data = fetal_health_test)

# Accuracy
acc_test_rf <- yardstick::accuracy(
  rf_test_preds,
  truth = b_fetal_health,
  estimate = .pred_class,
  event_level = "second"
)

# Sensitivity
sens_test_rf <- yardstick::sens(
  rf_test_preds,
  truth = b_fetal_health,
  estimate = .pred_class,
  event_level = "second"
)

# Specificity
spec_test_rf <- yardstick::spec(
  rf_test_preds,
  truth = b_fetal_health,
  estimate = .pred_class,
  event_level = "second"
)

# Combine into results tibble
stats_test_rf <- bind_rows(acc_test_rf, sens_test_rf, spec_test_rf) %>%
  mutate(data = "test", model = "rf")

# Confusion matrix
cm_test_rf <- yardstick::conf_mat(
  rf_test_preds,
  truth = b_fetal_health,
  estimate = .pred_class
)

stats_test_rf
cm_test_rf
```

How do test and train performance compare?

> The training performance is extremely high with accurancy, sensitivity and specificity value at 0.996, 0.987 and 0.998 respectively. The test performance is also strong with accurancy, sensitivity and specificity value at 0.948, 0.821 and 0.985 respectively. Since the sensitivity is at 0.82, the model does not detect abnormal cases as perfectly in new data.

```{r}
rm(rf_fit)
```

## Final model comparisons

So, if you had to recommend a model to consider using in practice, which of the models (if any) would you recommend and why?

> The logistic regression models worked fairly well but tended to miss more abnormal cases. The decision tree was easy to understand, but its accuracy wasn’t as strong. The Random Forest model handled the data better, gave higher accuracy on new data, and found more of the abnormal cases. Overall, it provides the best balance of performance and reliability.

> Out of all the models we tested, the Random Forest model is the one I would choose. It did the best job overall, especially when it came to catching the abnormal fetal cases. Since identifying those cases is the most important part, the Random Forest model feels like the safest and most reliable option.